import torch
import torch.multiprocessing as mp
import yaml
import time
from executor import worker

def load_config(config_path="config.yaml"):
    with open(config_path, "r") as f:
        return yaml.safe_load(f)

def main():
    config = load_config()
    gpu_count = torch.cuda.device_count()
    
    test_prompts = [
        "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
        "The moon is",
        "Beijing is the capital of"
    ]
    num_requests = len(test_prompts)

    print(f"ğŸš€ Starting VLLMLite with {gpu_count} GPUs.")
    print(f"ğŸ“¢ Total requests: {num_requests}\n")

    ctx = mp.get_context('spawn')
    result_queue = ctx.Queue()

    if gpu_count > 1:
        processes = mp.spawn(
            worker,
            args=(gpu_count, config, test_prompts, result_queue),
            nprocs=gpu_count,
            join=False
        )
    else:
        p = ctx.Process(target=worker, args=(0, 1, config, test_prompts, result_queue))
        p.start()

    # --- ç›‘å¬å¹¶åªæ˜¾ç¤º AI çš„å›ç­” ---
    completed = 0
    while completed < num_requests:
        if not result_queue.empty():
            req_id, ai_text = result_queue.get()
            
            if req_id == "DONE":
                break
            
            completed += 1
            # æ ¼å¼åŒ–è¾“å‡ºï¼šå»æ‰æ¢è¡Œç¬¦ä»¥ä¾¿åœ¨é¢„è§ˆä¸­æŸ¥çœ‹
            clean_text = ai_text.replace('\n', ' ')
            print(f"âœ… {req_id} å®Œæˆï¼")
            print(f"ğŸ¤– AI å›ç­”: {clean_text[:200]}...")
            print("-" * 60)
        else:
            time.sleep(0.1)

    print(f"\nâœ¨ æ‰€æœ‰ {completed} ä¸ªä»»åŠ¡å·²å¤„ç†å®Œæ¯•ã€‚")

    if gpu_count > 1:
        processes.join()

if __name__ == "__main__":
    main()
